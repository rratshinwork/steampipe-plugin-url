package url

import (
	// "compress/gzip"
	"context"
	"encoding/csv"
	"fmt"
	"io"
	// "os"
	"strings"

	// "github.com/dimchansky/utfbom"
	"github.com/turbot/go-kit/helpers"
	"github.com/turbot/steampipe-plugin-sdk/v5/grpc/proto"
	"github.com/turbot/steampipe-plugin-sdk/v5/plugin"
	// "github.com/turbot/steampipe-plugin-sdk/v5/connection"
	"github.com/turbot/steampipe-plugin-sdk/v5/plugin/transform"
	// "github.com/hashicorp/go-hclog"
	// "github.com/turbot/steampipe-plugin-sdk/v5/grpc/proto"

	"net/http"
	// "github.com/davecgh/go-spew/spew"
	// "sync"
	// "time"
	// "strconv"
	// "regexp"
	// "unicode"
)

// var sa_col_names []string
// func readFile(ctx context.Context, url string) (*csv.Reader, error) {

// 	file, err := os.Open(url)
// 	if err != nil {
// 		plugin.Logger(ctx).Error("url.readData", "os_open_error", err, "url", url)
// 		return nil, err
// 	}

// 	var dataFile io.Reader
// 	// if strings.HasSuffix(url, gzipExtension) {
// 	// 	gzipFile, err := gzip.NewReader(file)
// 	// 	if err != nil {
// 	// 		plugin.Logger(ctx).Error("url.readData", "gzip_open_error", err, "url", url)
// 	// 		return nil, err
// 	// 	}
// 	// 	dataFile = gzipFile
// 	// } else {
// 	// 	dataFile = file
// 	// }

// 	// Some CSV files have a non-standard Byte Order Mark (BOM) at the start
// 	// of the file - for example, UTF-8 encoded CSV files from Excel. This
// 	// messes up the first column name, so skip the BOM if found.
// 	dataFileWithoutBom, enc := utfbom.Skip(dataFile)
// 	plugin.Logger(ctx).Debug("url.tableData", "url", url, "detected_encoding", enc)

// 	r := csv.NewReader(dataFileWithoutBom)

// 	// dataConfig := GetConfig(connection)

// 	s1 := spew.Sdump(ctx)
// 	plugin.Logger(ctx).Warn("ctx -- " + s1)

// 	// s0 := spew.Sdump(dataConfig)
// 	// plugin.Logger(ctx).Warn("GETCONFIG -- " + s0)

// 	// if dataConfig.Separator != nil && *dataConfig.Separator != "" {
// 	// 	r.Comma = rune((*dataConfig.Separator)[0])
// 	// }
// 	// if dataConfig.Comment != nil {
// 	// 	if *dataConfig.Comment == "" {
// 	// 		// Disable comments
// 	// 		r.Comment = 0
// 	// 	} else {
// 	// 		// Set the comment character
// 	// 		r.Comment = rune((*dataConfig.Comment)[0])
// 	// 	}
// 	// }


// 	return r, nil
// }



func tableData(ctx context.Context, connection *plugin.Connection) (*plugin.Table) {


	// plugin1 := &plugin.Plugin{}
	// s2 := spew.Sdump(plugin1)
	// plugin.Logger(ctx).Warn("plugin1 -- " + s2)

	// logger := hclog.NewNullLogger()
	// log.SetOutput(logger.StandardWriter(&hclog.StandardLoggerOptions{InferLevels: true}))

	// plugin.initialise(logger)

	urlConfig := GetConfig(connection)
	var dataURL string
	if urlConfig.DataURL != nil {
		dataURL = *urlConfig.DataURL
		// plugin.Logger(ctx).Warn("dataURL -- " + dataURL)
	}

 	
	// // Read the header to peek at the column names
	// header, err := r.Read()
	// if err != nil {
	// 	// Parse errors
	// 	if len(header) > 0 {
	// 		plugin.Logger(ctx).Error("url.tableData", "header_parse_error", err, "url", url, "header", header)
	// 		// return nil, fmt.Errorf("failed to parse file header %s: %v", url, err)
	// 		return nil
	// 	}

	// 	// Return nil if the given file is empty, also add a log message to inform that the file is empty
	// 	plugin.Logger(ctx).Warn("url.tableData", "skipping the file since empty", url)
	// 	// return nil, nil
	// 	return nil
	// }

	// // Determine whether to use the first row as the header row when creating column names:
	// // - "auto": If there are no empty or duplicate values use the first row as the header. Else, use generic column names, e.g., "a", "b".
	// // - "on": Use the first row as the header. If there are empty or duplicate values, the tables will fail to load.
	// // - "off": Do not use the first row as the header. All column names will be generic.
	// // dataConfig := GetConfig(connection)
	// headerMode := "auto"


	cols := []*plugin.Column{}
	// colNames := []string{}
	// var headerValue string

	// // TODO: Can we read the header just once, collecting column names and rows
	// // along the way?

	// // If header mode is "off", no need to check if header is valid since it's
	// // not used
	// var isValidHeader bool
	// var invalidReason string
	// if headerMode == "auto" || headerMode == "on" {
	// 	isValidHeader, invalidReason = validHeader(ctx, header)
	// }

	// useHeaderRow := true

	// // Check if we should use header row
	// switch headerMode {
	// case "auto":
	// 	if !isValidHeader {
	// 		useHeaderRow = false
	// 	}
	// case "off":
	// 	useHeaderRow = false
	// case "on":
	// 	if !isValidHeader {
	// 		plugin.Logger(ctx).Error("url.tableData", "invalid_header_error", invalidReason, "url", url)
	// 		// return nil, fmt.Errorf(invalidReason)
	// 		return nil
	// 	}
	// }

	// for idx, i := range header {
	// 	if useHeaderRow {
	// 		headerValue = i
	// 	} else {
	// 		headerValue = intToLetters(idx + 1)
	// 	}


	// var sa_col_names []string
	// sa_col_names = append(sa_col_names, "url")
	// sa_col_names = append(sa_col_names, "row")

	// for idx, s_column_name := range sa_col_names {
	// 	cols = append(cols, &plugin.Column{Name: s_column_name, Type: proto.ColumnType_STRING, Transform: transform.FromField(helpers.EscapePropertyName(s_column_name)), Description: fmt.Sprintf("Field %d.", idx)})
	// }

	_, sa_column_map, _ := readData(ctx, dataURL)
	for s_column_name, s_column_type := range sa_column_map {
		if s_column_type == "INTEGER" {
			cols = append(cols, &plugin.Column{Name: s_column_name, Type: proto.ColumnType_INT, Transform: transform.FromField(helpers.EscapePropertyName(s_column_name))})
		} else if s_column_type == "NUMERIC" {
			cols = append(cols, &plugin.Column{Name: s_column_name, Type: proto.ColumnType_DOUBLE, Transform: transform.FromField(helpers.EscapePropertyName(s_column_name))})
		} else if s_column_type == "DATE" || s_column_type == "TIMESTAMP" {
			cols = append(cols, &plugin.Column{Name: s_column_name, Type: proto.ColumnType_TIMESTAMP, Transform: transform.FromField(helpers.EscapePropertyName(s_column_name))})
		} else {
			cols = append(cols, &plugin.Column{Name: s_column_name, Type: proto.ColumnType_STRING, Transform: transform.FromField(helpers.EscapePropertyName(s_column_name))})
		}
	}

	// s1 := spew.Sdump(cols)
	// plugin.Logger(ctx).Warn("cols -- " + s1)

	return &plugin.Table {
		Name: "data",
		List: &plugin.ListConfig{
			// Hydrate: listDataWithURL(sa_rows),
			Hydrate: listDataWithURL,
		},
		Columns: cols,
	}
}


func listDataWithURL (ctx context.Context, d *plugin.QueryData, h *plugin.HydrateData) (interface{}, error) {

		urlConfig := GetConfig(d.Connection)
		var dataURL string
		if urlConfig.DataURL != nil {
			dataURL = *urlConfig.DataURL
		}

		sa_rows, _, _ := readData(ctx, dataURL)
		for _, sm_row := range sa_rows {
			d.StreamListItem(ctx, sm_row)
		}

		return nil, nil
}

// func listDataWithURL (sa_rows []map[string]string) func(ctx context.Context, d *plugin.QueryData, h *plugin.HydrateData) (interface{}, error) {


// 	return func(ctx context.Context, d *plugin.QueryData, h *plugin.HydrateData) (interface{}, error) {

// 		// cols = append(*cols, &plugin.Column{Name: "yoyo", Type: proto.ColumnType_STRING, Transform: transform.FromField(helpers.EscapePropertyName("yoyo"))})

// 		// s1 := spew.Sdump(*cols)
// 		// plugin.Logger(ctx).Warn("cols -- " + s1)


// 		// quals := d.EqualsQuals
// 		// // s1 := spew.Sdump(quals)
// 		// s_url := quals["url"].GetStringValue()
// 		// plugin.Logger(ctx).Warn("EqualsQuals -- " + s_url)

// 		// r, err := readData(ctx, url)
// 		// if err != nil {
// 		// 	plugin.Logger(ctx).Error("url.listDataWithURL", "read_url_error", err, "url", url)
// 		// 	return nil, fmt.Errorf("failed to load Data file %s: %v", url, err)
// 		// }

// 		// // Header rows should not be used as a data row
// 		// if useHeaderRow {
// 		// 	header, err := r.Read()
// 		// 	if err != nil {
// 		// 		plugin.Logger(ctx).Error("url.listDataWithURL", "header_parse_error", err, "url", url, "header", header)
// 		// 		return nil, err
// 		// 	}
// 		// }

		

// 		// for {
// 		// 	record, err := r.Read()
// 		// 	if err == io.EOF {
// 		// 		break
// 		// 	}
// 		// 	if err != nil {
// 		// 		plugin.Logger(ctx).Error("url.listCSVWithPath", "record_parse_error", err, "path", path, "record", record)
// 		// 		continue
// 		// 	}
// 		// 	row := map[string]string{}
// 		// 	// for idx, j := range record {
// 		// 	for idx, _ := range record {
// 		// 		// row[colNames[idx]] = j
// 		// 		row[colNames[idx]] = "yoyo"
// 		// 		// plugin.Logger(ctx).Warn("row[colNames[idx]] -- " +  row[colNames[idx]])
// 		// 	}
// 		// 	d.StreamListItem(ctx, row)
// 		// }



// 		// r, _, _ := readData(ctx, s_url)
// 		// s1 := spew.Sdump(r)
// 		// plugin.Logger(ctx).Warn("r -- " + s1)
// 		// plugin.Logger(ctx).Warn("len(r) -- " + strconv.Itoa(len(r)))
// 		for _, sm_row := range sa_rows {
// 			// s2 := spew.Sdump(sm_row)
// 			// plugin.Logger(ctx).Warn("sm_row -- " + s2)
// 			d.StreamListItem(ctx, sm_row)
// 		}

// 		return nil, nil
// 	}
// }

// func listDataWithURL (s_url string, useHeader bool) func(ctx context.Context, d *plugin.QueryData, h *plugin.HydrateData) (interface{}, error) {

// 	return func(ctx context.Context, d *plugin.QueryData, h *plugin.HydrateData) (interface{}, error) {


// 		// quals := d.EqualsQuals
// 		// // s1 := spew.Sdump(quals)
// 		// s_url := quals["url"].GetStringValue()
// 		// plugin.Logger(ctx).Warn("EqualsQuals -- " + s_url)

// 		// r, err := readData(ctx, url)
// 		// if err != nil {
// 		// 	plugin.Logger(ctx).Error("url.listDataWithURL", "read_url_error", err, "url", url)
// 		// 	return nil, fmt.Errorf("failed to load Data file %s: %v", url, err)
// 		// }

// 		// // Header rows should not be used as a data row
// 		// if useHeaderRow {
// 		// 	header, err := r.Read()
// 		// 	if err != nil {
// 		// 		plugin.Logger(ctx).Error("url.listDataWithURL", "header_parse_error", err, "url", url, "header", header)
// 		// 		return nil, err
// 		// 	}
// 		// }

		

// 		// for {
// 		// 	record, err := r.Read()
// 		// 	if err == io.EOF {
// 		// 		break
// 		// 	}
// 		// 	if err != nil {
// 		// 		plugin.Logger(ctx).Error("url.listCSVWithPath", "record_parse_error", err, "path", path, "record", record)
// 		// 		continue
// 		// 	}
// 		// 	row := map[string]string{}
// 		// 	// for idx, j := range record {
// 		// 	for idx, _ := range record {
// 		// 		// row[colNames[idx]] = j
// 		// 		row[colNames[idx]] = "yoyo"
// 		// 		// plugin.Logger(ctx).Warn("row[colNames[idx]] -- " +  row[colNames[idx]])
// 		// 	}
// 		// 	d.StreamListItem(ctx, row)
// 		// }



// 		r, _, _ := readData(ctx, s_url)
// 		// s1 := spew.Sdump(r)
// 		// plugin.Logger(ctx).Warn("r -- " + s1)
// 		plugin.Logger(ctx).Warn("len(r) -- " + strconv.Itoa(len(r)))
// 		for _, sm_row := range r {
// 			// plugin.Logger(ctx).Warn("idx -- " + strconv.Itoa(idx))
// 			d.StreamListItem(ctx, sm_row)
// 		}

// 		return nil, nil
// 	}
// }



func readData(ctx context.Context, s_url string) ([]map[string]string, map[string]string, error) {

	var sa_data []map[string]string
	// var sa_rows [][] string
	var sa_columns [] string

	resp, err := http.Get(s_url)
    if err != nil {
        plugin.Logger(ctx).Error("readData Error < " + err.Error() + ">")
    }
    defer resp.Body.Close()

    var sb_data strings.Builder
    var i_buff_total int = 0
    var i_buff_max int = 20000000 // 20 MB
    var i_read_buff int = 1000000 // 1 MB
    var b_eof bool = false
    buff := make([]byte, i_read_buff)  
    for i_buff_total < i_buff_max {
        var bytesRead int  
        bytesRead, err = resp.Body.Read(buff)
        if err == io.EOF {
            b_eof = true
            if bytesRead <= 0 {  
                break  
            }
        }
        if err != nil {
        	plugin.Logger(ctx).Error("readData Error < " + err.Error() + " >")
        }
        s_data := string(buff[:bytesRead])
        // s_data,_ = ConvertString(s_data, "utf-8", "latin1")
        s_data = strings.Replace(s_data, "\r\n", "\n", -1) // handle DOS/Windows newlines
        s_data = strings.Replace(s_data, "\r", "\n", -1) // handle DOS/Windows newlines
        i_buff_total = len(sb_data.String())
        sb_data.WriteString(sanitizeUTF8(s_data))
    }

    s_final_data := sb_data.String()
    // plugin.Logger(ctx).Warn("s_final_data -- " + s_final_data)
    if b_eof == false {
        var i_final_newline int = strings.LastIndex(s_final_data, "\n")
        s_final_data = s_final_data[0 :i_final_newline]
    }

    // s_final_data = strings.Replace(s_final_data, "\r\n", "\n", -1) // handle DOS/Windows newlines


    detector := New()
	sampleLines := 4
	detector.Configure(&sampleLines, nil)
	delimiters := detector.DetectDelimiter(strings.NewReader(s_final_data), '"')
	// d1 := spew.Sdump(delimiters)
	// plugin.Logger(ctx).Warn("delimiters -- " + d1)

    nr := csv.NewReader(strings.NewReader(s_final_data))
    s_separator := strings.Replace(delimiters[0], "/", "//", -1)
    nr.Comma = GetSeparator(s_separator)

	records, err := nr.ReadAll()
	if err != nil {
		plugin.Logger(ctx).Error(err.Error())
	}

	for _, s_value := range records[0] {
		sa_columns = append(sa_columns, s_value)
	}

	sa_column_map := make(map[string]string)
	var s_column_types string
	for idx, s_column := range sa_columns {
		i_false_date := 0
		i_false_integer := 0
		i_false_numeric := 0
		s_data_type := "STRING"
		for idx0, sa_row := range records {
			if idx0 == 0 {
				continue;
			}
			s_value := sa_row[idx]
			if !isDate(s_value) {
				i_false_date++
			}
			if !isInteger(s_value) {
				i_false_integer++
			}
			if !isNumeric(s_value) {
				i_false_numeric++
			}
			if i_false_date > 0 && i_false_integer > 0 && i_false_numeric > 0 {
				break
			}
		}
		if i_false_date == 0 {
			s_data_type = "DATE"
		} else if i_false_integer == 0 {
			s_data_type = "INTEGER"
		} else if i_false_numeric == 0 {
			s_data_type = "NUMERIC"
		}
		
		sa_column_map[s_column] = s_data_type
		s_column_types += s_column + ":" + s_data_type + "|"
		// plugin.Logger(ctx).Warn(s_column + " [" + sa_column_map[s_column] + "]")
	}

	for idx, record := range records {
		if idx == 0 {
			continue
		}
		sm_row := map[string]string{}
		// sm_row["url"] = s_url
		// s_row := strings.Join(record, "|")
		// if strings.TrimSpace(s_row) != "" {
		// 	sm_row["row"] = s_row
		// }
		for idx0, s_value := range record {
			sm_row[sa_columns[idx0]] = s_value
		}
		// s1 := spew.Sdump(sm_row)
		// plugin.Logger(ctx).Warn("sm_row -- " + s1)
		sa_data = append(sa_data, sm_row)
	}

	
	// s_column_types = strings.TrimRight(s_column_types, "|")
	// sm_row := map[string]string{}
	// sm_row["url"] = s_url
	// sm_row["row"] = s_column_types
	// sa_data = append(sa_data, sm_row)
	return sa_data, sa_column_map, nil

}

// A valid header row has no empty values or duplicate values
func validHeader(ctx context.Context, header []string) (bool, string) {
	keys := make(map[string]bool)
	for idx, i := range header {
		// Check for empty column names
		if len(i) == 0 {
			return false, fmt.Sprintf("header row has empty value in field %d", idx)
		}
		// Check for duplicate column names
		_, ok := keys[i]
		if ok {
			return false, fmt.Sprintf("header row has duplicate value in field %d", idx)
		}
		keys[i] = true
	}

	// No empty or duplicate column names found
	return true, ""
}
